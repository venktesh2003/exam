PARALLEL DFS BFS
#include <iostream>
#include <vector>
#include <stack>
#include <omp.h>
#include<queue>

using namespace std;

struct Graph {
    int V;
    vector<vector<int>> adj;
};

void addEdge(Graph& graph, int u, int v) {
    graph.adj[u].push_back(v);
    graph.adj[v].push_back(u);
}
void parallelBFS(Graph& graph, int start) {
    int numVertices = graph.V;
    vector<bool> visited(numVertices, false);
    queue<int> q;

    visited[start] = true;
    q.push(start);

    while (!q.empty()) {
        int current;
        #pragma omp critical
        {
            current = q.front();
            q.pop();
        }

        cout << "BFS Visited: " << current << endl;

        #pragma omp parallel for
        for (int i = 0; i < graph.adj[current].size(); ++i) {
            int next = graph.adj[current][i];
            if (!visited[next]) {
                #pragma omp critical
                {
                    visited[next] = true;
                    q.push(next);
                }
            }
        }
    }
}


void parallelDFS(Graph& graph, int start) {
    int numVertices = graph.V;
    vector<bool> visited(numVertices, false);
    stack<int> stack;

    visited[start] = true;
    stack.push(start);

    #pragma omp parallel
    {
        while (!stack.empty()) {
            int current;
            #pragma omp critical
            {
                current = stack.top();
                stack.pop();
            }

            // Process current node
            #pragma omp critical
            {
                cout << "DFS Visited: " << current << endl;
            }

            // Parallelize the loop for neighbors
            #pragma omp for
            for (int i = 0; i < graph.adj[current].size(); ++i) {
                int next = graph.adj[current][i];
                if (!visited[next]) {
                    #pragma omp critical
                    {
                        visited[next] = true;
                        stack.push(next);
                    }
                }
            }
        }
    }
}

int main() {
    Graph graph;
    int numVertices = 6;
    graph.V = numVertices;
    graph.adj.resize(numVertices);

    addEdge(graph, 0, 1);
    addEdge(graph, 0, 2);
    addEdge(graph, 1, 3);
    addEdge(graph, 2, 4);
    addEdge(graph, 2, 5);

    cout << "Depth First Search:" << endl;
    parallelDFS(graph, 0);
    cout<<"Breadth first search"<<endl;
    parallelBFS(graph , 0);

    return 0;
}
===================================================================================================

PARALLEL MERGE SORT BUBBLE SORT

#include <bits/stdc++.h>
#include <omp.h>

using namespace std;

void merge(vector<int> &arr, int low, int mid, int high)
{
    vector<int> temp;    // temporary array
    int left = low;      // starting index of left half of arr
    int right = mid + 1; // starting index of right half of arr

    // storing elements in the temporary array in a sorted manner//

    while (left <= mid && right <= high)
    {
        if (arr[left] <= arr[right])
        {
            temp.push_back(arr[left]);
            left++;
        }
        else
        {
            temp.push_back(arr[right]);
            right++;
        }
    }

    // if elements on the left half are still left //

    while (left <= mid)
    {
        temp.push_back(arr[left]);
        left++;
    }

    //  if elements on the right half are still left //
    while (right <= high)
    {
        temp.push_back(arr[right]);
        right++;
    }

    // transfering all elements from temporary to arr //
    for (int i = low; i <= high; i++)
    {
        arr[i] = temp[i - low];
    }
}

void mergeSort(vector<int> &arr, int low, int high)
{
    int mid;

    if (low < high)
    {
        mid = (low + high) / 2;
        {
#pragma omp parallel sections
            {
#pragma omp parallel section
                mergeSort(arr, low, mid);
            }
#pragma omp parallel section

            {
                mergeSort(arr, mid + 1, high);
            }
            merge(arr, low, mid, high);
        }
    }

    // left half
    // right half
    // merging sorted halves
}


void bubble_sort(vector<int> &v)
{
    int n = v.size();
#pragma omp parallel for shared(v, n)
    for (int i = 0; i < n; i++)
    {
#pragma omp parallel for shared(v, n)
        for (int j = i + 1; j < n; j++)
        {
            if (v[i] > v[j])
            {
                swap(v[i], v[j]);
            }
        }
    }
}

int main()
{
    int n;
    cin >> n;
    vector<int> v(n);
    for (int i = 0; i < n; i++)
    {
        cin >> v[i];
    }
    mergeSort(v, 0 ,n-1);
    cout << "\n Sorted array is: " << endl;
    for (auto it : v)
    {
        cout << it << " ";
    }
}

==================================================================================================
MIN MAX AVG SUM PARALLEL REDUCTION

#include <bits/stdc++.h>
#include <omp.h>

using namespace std;

int sum_reduction(vector<int> &arr)
{
    int sum = 0;

#pragma omp parallel for reduction(+ : sum)
    for (int i = 0; i < arr.size(); i++)
    {
        sum += arr[i];
    }

    return sum;
}

int min_reduction(vector<int> &arr)
{
    int min_value = INT_MAX;

#pragma omp parallel for reduction(min : min_value)
    for (int i = 0; i < arr.size(); i++)
    {
        if (min_value > arr[i])
        {
            min_value = arr[i];
        }
    }

    return min_value;
}

int max_reduction(vector<int> &arr)
{
    int max_value = INT_MIN;

#pragma omp parallel for reduction(max : max_value)
    for (int i = 0; i < arr.size(); i++)
    {
        if (arr[i] > max_value)
        {
            max_value = arr[i];
        }
    }

    return max_value;
}

int avg_reduction(vector<int> &arr)
{
    int max_value = INT_MIN;

    int sum = 0;
    int n = arr.size();
#pragma omp parallel for reduction(+ : sum)
    for (int i = 0; i < arr.size(); i++)
    {
        sum += arr[i];
    }

    return (sum / n);
}

int main()
{
    int n;
    cout << "Enter the size of array: " << endl;
    cin >> n;
    vector<int> arr(n);
    cout << "Enter the elements in an array: " << endl;
    for (int i = 0; i < n; i++)
    {
        cin >> arr[i];
    }
    cout << "\nSun of array is: " << sum_reduction(arr) << endl;
    cout << "\nAverage of array is: " << avg_reduction(arr) << endl;
    cout << "\nMin value of array is: " << min_reduction(arr) << endl;
    cout << "\nMax value of array is: " << max_reduction(arr) << endl;
}
==================================================================================================
ADDITION OF VECTOR CUDA

#include <iostream>
#include <cuda_runtime.h>

using namespace std;

__global__ void addVectors(int *A, int *B, int *C, int n)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n)
    {
        C[i] = A[i] + B[i];
    }
}

int main()
{
    int n = 1000000;
    int *A, *B, *C;
    int size = n * sizeof(int);

    // Allocate memory on the host
    cudaMallocHost(&A, size);
    cudaMallocHost(&B, size);
    cudaMallocHost(&C, size);

    // Initialize the vectors
    for (int i = 0; i < n; i++)
    {
        A[i] = i;
        B[i] = i * 2;
    }

    // Allocate memory on the device
    int *dev_A, *dev_B, *dev_C;
    cudaMalloc(&dev_A, size);
    cudaMalloc(&dev_B, size);
    cudaMalloc(&dev_C, size);

    // Copy data from host to device
    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);

    // Launch the kernel
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;
    addVectors<<<numBlocks, blockSize>>>(dev_A, dev_B, dev_C, n);

    // Copy data from device to host
    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);

    // Print the results
    for (int i = 0; i < 10; i++)
    {
        cout << C[i] << " ";
    }
    cout << endl;

    // Free memory
    cudaFree(dev_A);
    cudaFree(dev_B);
    cudaFree(dev_C);
    cudaFreeHost(A);
    cudaFreeHost(B);
    cudaFreeHost(C);

    return 0;
}

/*
if gpu present
nvcc hello_world.cu -o hello_world
./hello_world
*/

==================================================================================================
MATRIX MULTIPLICATION CUDA

#include <cuda_runtime.h>
#include <iostream>
__global__ void matmul(int *A, int *B, int *C, int N)
{
    int Row = blockIdx.y * blockDim.y + threadIdx.y;
    int Col = blockIdx.x * blockDim.x + threadIdx.x;
    if (Row < N && Col < N)
    {
        int Pvalue = 0;
        for (int k = 0; k < N; k++)
        {
            Pvalue += A[Row * N + k] * B[k * N + Col];
        }
        C[Row * N + Col] = Pvalue;
    }
}
int main()
{
    int N = 512;
    int size = N * N * sizeof(int);
    int *A, *B, *C;
    int *dev_A, *dev_B, *dev_C;
    cudaMallocHost(&A, size);
    cudaMallocHost(&B, size);
    cudaMallocHost(&C, size);
    cudaMalloc(&dev_A, size);
    cudaMalloc(&dev_B, size);
    cudaMalloc(&dev_C, size);
    // Initialize matrices A and B
    for (int i = 0; i < N; i++)
    {
        for (int j = 0; j < N; j++)
        {
            A[i * N + j] = i * N + j;
            B[i * N + j] = j * N + i;
        }
    }
    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);
    dim3 dimBlock(16, 16);
    dim3 dimGrid(N / dimBlock.x, N / dimBlock.y);
    matmul<<<dimGrid, dimBlock>>>(dev_A, dev_B, dev_C, N);
    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);
    // Print the result
    for (int i = 0; i < 10; i++)
    {
        for (int j = 0; j < 10; j++)
        {
            std::cout << C[i * N + j] << " ";
        }
        std::cout << std::endl;
    }
    // Free memory
    cudaFree(dev_A);
    cudaFree(dev_B);
    cudaFree(dev_C);
    cudaFreeHost(A);
    cudaFreeHost(B);
    cudaFreeHost(C);
    return 0;
}

/*
if gpu present
nvcc hello_world.cu -o hello_world
./hello_world
*/
===================================================================================================